{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Speech Enhancement using SEGANS\n",
    "\n",
    "This notebook implements a deep learning approach for speech enhancement using **SEGANS (Speech Enhancement Generative Adversarial Networks)**. The project is structured into multiple components to ensure modularity and maintainability. Each section of the notebook corresponds to a specific part of the project.\n",
    "\n",
    "## Project Overview\n",
    "Speech enhancement aims to improve the quality and intelligibility of speech signals by reducing noise or other distortions. **SEGANS** leverages the power of Generative Adversarial Networks (GANs) to perform this task effectively. The workflow includes:\n",
    "1. **Preprocessing**: Preparing the dataset by normalizing and transforming audio signals into a suitable format for training.\n",
    "2. **Model Development**: Designing the SEGANS architecture, including generator and discriminator networks.\n",
    "3. **Training**: Training the model using the preprocessed data to minimize loss and enhance performance.\n",
    "4. **Testing**: Evaluating the model on unseen data and analyzing its performance.\n",
    "\n",
    "This notebook is organized into the following sections:\n",
    "- **Preprocessing**: Code for loading, cleaning, and preparing audio data.\n",
    "- **Model**: Definition of the SEGANS model, including the generator and discriminator.\n",
    "- **Training**: Implementation of the training loop, along with loss functions and optimizers.\n",
    "- **Testing**: Validation and performance metrics on the test dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing for Speech Enhancement\n",
    "\n",
    "The preprocessing step prepares audio data for the SEGANS model by performing the following tasks:\n",
    "1. **Audio Segmentation**: Audio files are divided into fixed-sized segments with overlapping windows. This segmentation ensures that each audio file is broken into smaller, manageable chunks for training and testing.\n",
    "2. **Serialization**: Clean and noisy audio segments are paired and saved as `.npy` files for efficient use during training and testing.\n",
    "3. **Validation**: Ensures that the serialized data matches the expected format and segment length.\n",
    "\n",
    "### Key Parameters:\n",
    "- **Window Size**: 2^14 samples (approx. 1 second of audio).\n",
    "- **Sample Rate**: 16 kHz (target sample rate for audio processing).\n",
    "- **Stride**: 50% overlap between consecutive segments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import librosa\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.modules import Module\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.utils import data\n",
    "import argparse\n",
    "from scipy.io import wavfile\n",
    "from torch import optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory Paths\n",
    "DIRS = {\n",
    "    \"train_clean\": 'data/clean_trainset',\n",
    "    \"train_noisy\": 'data/noisy_trainset',\n",
    "    \"test_clean\": 'data/clean_testset',\n",
    "    \"test_noisy\": 'data/noisy_testset',\n",
    "    \"train_serialized\": 'data/serialized_train_data',\n",
    "    \"test_serialized\": 'data/serialized_test_data',\n",
    "}\n",
    "\n",
    "# Parameters\n",
    "WINDOW_SIZE = 2 ** 14  # Approx. 1 second\n",
    "SAMPLE_RATE = 16000\n",
    "STRIDE = 0.5  # 50% overlap\n",
    "\n",
    "\n",
    "def segment_audio_file(audio_path, window_size=WINDOW_SIZE, stride=STRIDE, sample_rate=SAMPLE_RATE):\n",
    "    \"\"\"\n",
    "    Segments an audio file into overlapping segments.\n",
    "\n",
    "    Args:\n",
    "        audio_path (str): Path to the audio file.\n",
    "        window_size (int): Number of samples per segment.\n",
    "        stride (float): Overlap ratio (e.g., 0.5 for 50% overlap).\n",
    "        sample_rate (int): Target sample rate for audio.\n",
    "\n",
    "    Returns:\n",
    "        list: List of audio segments.\n",
    "    \"\"\"\n",
    "    audio, sr = librosa.load(audio_path, sr=sample_rate)\n",
    "    hop_length = int(window_size * stride)\n",
    "    return [audio[i:i + window_size] for i in range(0, len(audio) - window_size + 1, hop_length)]\n",
    "\n",
    "\n",
    "def preprocess_and_save(dataset_type=\"train\"):\n",
    "    \"\"\"\n",
    "    Processes and serializes audio data (clean and noisy) for training or testing.\n",
    "\n",
    "    Args:\n",
    "        dataset_type (str): Type of dataset ('train' or 'test').\n",
    "    \"\"\"\n",
    "    clean_path = DIRS[f\"{dataset_type}_clean\"]\n",
    "    noisy_path = DIRS[f\"{dataset_type}_noisy\"]\n",
    "    save_path = DIRS[f\"{dataset_type}_serialized\"]\n",
    "\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "    print(f\"Processing {dataset_type} data...\")\n",
    "    for filename in tqdm(os.listdir(clean_path), desc=f\"Serializing {dataset_type}\"):\n",
    "        clean_file = os.path.join(clean_path, filename)\n",
    "        noisy_file = os.path.join(noisy_path, filename)\n",
    "\n",
    "        # Segment clean and noisy files\n",
    "        clean_segments = segment_audio_file(clean_file)\n",
    "        noisy_segments = segment_audio_file(noisy_file)\n",
    "\n",
    "        # Save paired segments as .npy\n",
    "        for idx, (clean_segment, noisy_segment) in enumerate(zip(clean_segments, noisy_segments)):\n",
    "            pair = np.array([clean_segment, noisy_segment])\n",
    "            save_filename = f\"{os.path.splitext(filename)[0]}_{idx}.npy\"\n",
    "            np.save(os.path.join(save_path, save_filename), pair)\n",
    "\n",
    "\n",
    "def validate_serialized_data(dataset_type=\"train\"):\n",
    "    \"\"\"\n",
    "    Validates the serialized data by checking the consistency of segment lengths.\n",
    "\n",
    "    Args:\n",
    "        dataset_type (str): Type of dataset ('train' or 'test').\n",
    "    \"\"\"\n",
    "    serialized_path = DIRS[f\"{dataset_type}_serialized\"]\n",
    "    print(f\"Validating {dataset_type} serialized data...\")\n",
    "\n",
    "    for file in tqdm(os.listdir(serialized_path), desc=f\"Validating {dataset_type}\"):\n",
    "        file_path = os.path.join(serialized_path, file)\n",
    "        data_pair = np.load(file_path)\n",
    "\n",
    "        # Validate shape\n",
    "        if data_pair.shape[1] != WINDOW_SIZE:\n",
    "            print(f\"Error: Segment in {file} does not match window size {WINDOW_SIZE}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Audio Dataset \n",
    "\n",
    "In this section, we define the `AudioDataset` class and the `emphasis` function, which are essential for handling and processing audio data.\n",
    "\n",
    "- **`emphasis`**: This function applies pre-emphasis or de-emphasis filtering to audio signals. It is important for accentuating or diminishing certain frequency ranges to enhance learning and reduce noise in the data.\n",
    "  \n",
    "- **`AudioDataset`**: This custom PyTorch dataset class is responsible for loading the audio data (both clean and noisy), applying the emphasis function, and organizing the data into batches. The dataset also provides a method for generating reference batches used during virtual batch normalization.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def emphasis(signal_batch, emph_coeff=0.95, pre=True):\n",
    "    \"\"\"\n",
    "    Pre-emphasis or De-emphasis of higher frequencies given a batch of signal.\n",
    "\n",
    "    Args:\n",
    "        signal_batch: batch of signals, represented as numpy arrays\n",
    "        emph_coeff: emphasis coefficient\n",
    "        pre: pre-emphasis or de-emphasis signals\n",
    "\n",
    "    Returns:\n",
    "        result: pre-emphasized or de-emphasized signal batch\n",
    "    \"\"\"\n",
    "    result = np.zeros(signal_batch.shape)\n",
    "    for sample_idx, sample in enumerate(signal_batch):\n",
    "        for ch, channel_data in enumerate(sample):\n",
    "            if pre:\n",
    "                result[sample_idx][ch] = np.append(channel_data[0], channel_data[1:] - emph_coeff * channel_data[:-1])\n",
    "            else:\n",
    "                result[sample_idx][ch] = np.append(channel_data[0], channel_data[1:] + emph_coeff * channel_data[:-1])\n",
    "    return result\n",
    "\n",
    "\n",
    "class AudioDataset(data.Dataset):\n",
    "    \"\"\"\n",
    "    Audio sample reader using preprocessed and serialized data.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data_type):\n",
    "        \"\"\"\n",
    "        Initializes the dataset by pointing to the respective serialized folder.\n",
    "\n",
    "        Args:\n",
    "            data_type (str): Type of dataset ('train' or 'test').\n",
    "        \"\"\"\n",
    "        # Use serialized data directory paths from the `DIRS` dictionary\n",
    "        if data_type == 'train':\n",
    "            data_path = DIRS['train_serialized']\n",
    "        else:\n",
    "            data_path = DIRS['test_serialized']\n",
    "\n",
    "        if not os.path.exists(data_path):\n",
    "            raise FileNotFoundError(f'The {data_type} serialized data folder does not exist!')\n",
    "\n",
    "        self.data_type = data_type\n",
    "        self.file_names = [os.path.join(data_path, filename) for filename in os.listdir(data_path)]\n",
    "\n",
    "    def reference_batch(self, batch_size):\n",
    "        \"\"\"\n",
    "        Randomly selects a reference batch from dataset.\n",
    "        Reference batch is used for calculating statistics for virtual batch normalization operation.\n",
    "\n",
    "        Args:\n",
    "            batch_size (int): batch size\n",
    "\n",
    "        Returns:\n",
    "            ref_batch: reference batch\n",
    "        \"\"\"\n",
    "        ref_file_names = np.random.choice(self.file_names, batch_size)\n",
    "        ref_batch = np.stack([np.load(f) for f in ref_file_names])\n",
    "\n",
    "        ref_batch = emphasis(ref_batch, emph_coeff=0.95)\n",
    "        return torch.from_numpy(ref_batch).type(torch.FloatTensor)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Retrieves a single data sample from the dataset.\n",
    "\n",
    "        Args:\n",
    "            idx (int): Index of the sample.\n",
    "\n",
    "        Returns:\n",
    "            tuple: (pair, clean, noisy) for training, or (file_name, noisy) for testing.\n",
    "        \"\"\"\n",
    "        pair = np.load(self.file_names[idx])\n",
    "        pair = emphasis(pair[np.newaxis, :, :], emph_coeff=0.95).reshape(2, -1)\n",
    "        noisy = pair[1].reshape(1, -1)\n",
    "\n",
    "        if self.data_type == 'train':\n",
    "            clean = pair[0].reshape(1, -1)\n",
    "            return torch.from_numpy(pair).type(torch.FloatTensor), torch.from_numpy(clean).type(\n",
    "                torch.FloatTensor), torch.from_numpy(noisy).type(torch.FloatTensor)\n",
    "        else:\n",
    "            return os.path.basename(self.file_names[idx]), torch.from_numpy(noisy).type(torch.FloatTensor)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns the total number of samples in the dataset.\n",
    "\n",
    "        Returns:\n",
    "            int: Total number of samples.\n",
    "        \"\"\"\n",
    "        return len(self.file_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now we go to implement the model.\n",
    "\n",
    "The model consists of two main components:\n",
    "1. **Generator (G):** Generates synthetic signals from random latent vectors (`z`). It is trained to produce outputs that resemble real signals.\n",
    "2. **Discriminator (D):** Distinguishes between real signals and synthetic signals produced by the generator. It provides feedback to the generator to improve its outputs.\n",
    "\n",
    "Key features of the model:\n",
    "- **1D Convolutions:** Efficiently process sequential data for both generator and discriminator.\n",
    "- **Batch Normalization (BN):** Reduces batch dependence during training, improving the stability and generalization of the model.\n",
    "- **Leaky ReLU and Tanh Activations:** Introduce non-linearities to capture complex data patterns.\n",
    "\n",
    "The training goal is to achieve a balance where the generator produces realistic signals, making it difficult for the discriminator to differentiate between real and fake samples.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BatchNormlization Class\n",
    " It implements the Virtual Batch Normalization (BN) layer.\n",
    "\n",
    "Key features:\n",
    "- Learns scale (`gamma`) and shift (`beta`) parameters.\n",
    "- Computes batch statistics for normalization.\n",
    "- Updates reference statistics to normalize inputs during training.\n",
    "- Improves generalization in GANs by reducing batch dependence.\n",
    "\n",
    "References:\n",
    "- Virtual Batch Normalization: https://arxiv.org/abs/1606.03498\n",
    "- Implementation discussion: https://discuss.pytorch.org/t/parameter-grad-of-conv-weight-is-none-after-virtual-batch-normalization/9036\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VirtualBatchNorm1d(Module):\n",
    "    def __init__(self, num_features, eps=1e-5):\n",
    "        super().__init__()\n",
    "        self.num_features = num_features\n",
    "        self.eps = eps\n",
    "        self.gamma = Parameter(torch.normal(mean=1.0, std=0.02, size=(1, num_features, 1)))\n",
    "        self.beta = Parameter(torch.zeros(1, num_features, 1))\n",
    "\n",
    "    def get_stats(self, x):\n",
    "        mean = x.mean(2, keepdim=True).mean(0, keepdim=True)\n",
    "        mean_sq = (x ** 2).mean(2, keepdim=True).mean(0, keepdim=True)\n",
    "        return mean, mean_sq\n",
    "\n",
    "    def forward(self, x, ref_mean, ref_mean_sq):\n",
    "        mean, mean_sq = self.get_stats(x)\n",
    "        if ref_mean is None or ref_mean_sq is None:\n",
    "            mean = mean.clone().detach()\n",
    "            mean_sq = mean_sq.clone().detach()\n",
    "            out = self.normalize(x, mean, mean_sq)\n",
    "        else:\n",
    "            batch_size = x.size(0)\n",
    "            new_coeff = 1. / (batch_size + 1.)\n",
    "            old_coeff = 1. - new_coeff\n",
    "            mean = new_coeff * mean + old_coeff * ref_mean\n",
    "            mean_sq = new_coeff * mean_sq + old_coeff * ref_mean_sq\n",
    "            out = self.normalize(x, mean, mean_sq)\n",
    "        return out, mean, mean_sq\n",
    "\n",
    "    def normalize(self, x, mean, mean_sq):\n",
    "        std = torch.sqrt(self.eps + mean_sq - mean ** 2)\n",
    "        x = x - mean\n",
    "        x = x / std\n",
    "        x = x * self.gamma\n",
    "        x = x + self.beta\n",
    "        return x\n",
    "\n",
    "    def __repr__(self):\n",
    "        return ('{name}(num_features={num_features}, eps={eps}'\n",
    "                .format(name=self.__class__.__name__, **self.__dict__))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator Class\n",
    "is  the core of the GAN's generator.\n",
    "\n",
    "Key features:\n",
    "- Implements a neural network that generates signals from a latent vector `z`.\n",
    "- Contains convolutional and transposed convolutional layers for encoding and decoding signals.\n",
    "- Uses activation functions (`PReLU` and `Tanh`) to introduce non-linearity.\n",
    "- Initializes weights using Xavier initialization for better convergence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    \"\"\"G\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # encoder gets a noisy signal as input [B x 1 x 16384]\n",
    "        self.enc1 = nn.Conv1d(in_channels=1, out_channels=16, kernel_size=32, stride=2, padding=15)  # [B x 16 x 8192]\n",
    "        self.enc1_nl = nn.PReLU()\n",
    "        self.enc2 = nn.Conv1d(16, 32, 32, 2, 15)  # [B x 32 x 4096]\n",
    "        self.enc2_nl = nn.PReLU()\n",
    "        self.enc3 = nn.Conv1d(32, 32, 32, 2, 15)  # [B x 32 x 2048]\n",
    "        self.enc3_nl = nn.PReLU()\n",
    "        self.enc4 = nn.Conv1d(32, 64, 32, 2, 15)  # [B x 64 x 1024]\n",
    "        self.enc4_nl = nn.PReLU()\n",
    "        self.enc5 = nn.Conv1d(64, 64, 32, 2, 15)  # [B x 64 x 512]\n",
    "        self.enc5_nl = nn.PReLU()\n",
    "        self.enc6 = nn.Conv1d(64, 128, 32, 2, 15)  # [B x 128 x 256]\n",
    "        self.enc6_nl = nn.PReLU()\n",
    "        self.enc7 = nn.Conv1d(128, 128, 32, 2, 15)  # [B x 128 x 128]\n",
    "        self.enc7_nl = nn.PReLU()\n",
    "        self.enc8 = nn.Conv1d(128, 256, 32, 2, 15)  # [B x 256 x 64]\n",
    "        self.enc8_nl = nn.PReLU()\n",
    "        self.enc9 = nn.Conv1d(256, 256, 32, 2, 15)  # [B x 256 x 32]\n",
    "        self.enc9_nl = nn.PReLU()\n",
    "        self.enc10 = nn.Conv1d(256, 512, 32, 2, 15)  # [B x 512 x 16]\n",
    "        self.enc10_nl = nn.PReLU()\n",
    "        self.enc11 = nn.Conv1d(512, 1024, 32, 2, 15)  # [B x 1024 x 8]\n",
    "        self.enc11_nl = nn.PReLU()\n",
    "\n",
    "        # decoder generates an enhanced signal\n",
    "        self.dec10 = nn.ConvTranspose1d(in_channels=2048, out_channels=512, kernel_size=32, stride=2, padding=15)\n",
    "        self.dec10_nl = nn.PReLU()  # out : [B x 512 x 16] -> (concat) [B x 1024 x 16]\n",
    "        self.dec9 = nn.ConvTranspose1d(1024, 256, 32, 2, 15)  # [B x 256 x 32]\n",
    "        self.dec9_nl = nn.PReLU()\n",
    "        self.dec8 = nn.ConvTranspose1d(512, 256, 32, 2, 15)  # [B x 256 x 64]\n",
    "        self.dec8_nl = nn.PReLU()\n",
    "        self.dec7 = nn.ConvTranspose1d(512, 128, 32, 2, 15)  # [B x 128 x 128]\n",
    "        self.dec7_nl = nn.PReLU()\n",
    "        self.dec6 = nn.ConvTranspose1d(256, 128, 32, 2, 15)  # [B x 128 x 256]\n",
    "        self.dec6_nl = nn.PReLU()\n",
    "        self.dec5 = nn.ConvTranspose1d(256, 64, 32, 2, 15)  # [B x 64 x 512]\n",
    "        self.dec5_nl = nn.PReLU()\n",
    "        self.dec4 = nn.ConvTranspose1d(128, 64, 32, 2, 15)  # [B x 64 x 1024]\n",
    "        self.dec4_nl = nn.PReLU()\n",
    "        self.dec3 = nn.ConvTranspose1d(128, 32, 32, 2, 15)  # [B x 32 x 2048]\n",
    "        self.dec3_nl = nn.PReLU()\n",
    "        self.dec2 = nn.ConvTranspose1d(64, 32, 32, 2, 15)  # [B x 32 x 4096]\n",
    "        self.dec2_nl = nn.PReLU()\n",
    "        self.dec1 = nn.ConvTranspose1d(64, 16, 32, 2, 15)  # [B x 16 x 8192]\n",
    "        self.dec1_nl = nn.PReLU()\n",
    "        self.dec_final = nn.ConvTranspose1d(32, 1, 32, 2, 15)  # [B x 1 x 16384]\n",
    "        self.dec_tanh = nn.Tanh()\n",
    "\n",
    "        # initialize weights\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        \"\"\"\n",
    "        Initialize weights for convolution layers using Xavier initialization.\n",
    "        \"\"\"\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv1d) or isinstance(m, nn.ConvTranspose1d):\n",
    "                nn.init.xavier_normal(m.weight.data)\n",
    "\n",
    "    def forward(self, x, z):\n",
    "        \"\"\"\n",
    "        Forward pass of generator.\n",
    "\n",
    "        Args:\n",
    "            x: input batch (signal)\n",
    "            z: latent vector\n",
    "        \"\"\"\n",
    "        # encoding step\n",
    "        e1 = self.enc1(x)\n",
    "        e2 = self.enc2(self.enc1_nl(e1))\n",
    "        e3 = self.enc3(self.enc2_nl(e2))\n",
    "        e4 = self.enc4(self.enc3_nl(e3))\n",
    "        e5 = self.enc5(self.enc4_nl(e4))\n",
    "        e6 = self.enc6(self.enc5_nl(e5))\n",
    "        e7 = self.enc7(self.enc6_nl(e6))\n",
    "        e8 = self.enc8(self.enc7_nl(e7))\n",
    "        e9 = self.enc9(self.enc8_nl(e8))\n",
    "        e10 = self.enc10(self.enc9_nl(e9))\n",
    "        e11 = self.enc11(self.enc10_nl(e10))\n",
    "        # c = compressed feature, the 'thought vector'\n",
    "        c = self.enc11_nl(e11)\n",
    "        \n",
    "        encoded = torch.cat((c, z), dim=1)\n",
    "\n",
    "        # decoding step\n",
    "        d10 = self.dec10(encoded)\n",
    "        # dx_c : concatenated with skip-connected layer's output & passed nonlinear layer\n",
    "        d10_c = self.dec10_nl(torch.cat((d10, e10), dim=1))\n",
    "        d9 = self.dec9(d10_c)\n",
    "        d9_c = self.dec9_nl(torch.cat((d9, e9), dim=1))\n",
    "        d8 = self.dec8(d9_c)\n",
    "        d8_c = self.dec8_nl(torch.cat((d8, e8), dim=1))\n",
    "        d7 = self.dec7(d8_c)\n",
    "        d7_c = self.dec7_nl(torch.cat((d7, e7), dim=1))\n",
    "        d6 = self.dec6(d7_c)\n",
    "        d6_c = self.dec6_nl(torch.cat((d6, e6), dim=1))\n",
    "        d5 = self.dec5(d6_c)\n",
    "        d5_c = self.dec5_nl(torch.cat((d5, e5), dim=1))\n",
    "        d4 = self.dec4(d5_c)\n",
    "        d4_c = self.dec4_nl(torch.cat((d4, e4), dim=1))\n",
    "        d3 = self.dec3(d4_c)\n",
    "        d3_c = self.dec3_nl(torch.cat((d3, e3), dim=1))\n",
    "        d2 = self.dec2(d3_c)\n",
    "        d2_c = self.dec2_nl(torch.cat((d2, e2), dim=1))\n",
    "        d1 = self.dec1(d2_c)\n",
    "        d1_c = self.dec1_nl(torch.cat((d1, e1), dim=1))\n",
    "        out = self.dec_tanh(self.dec_final(d1_c))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discriminator Class\n",
    "is the counterpart to the `Generator`.\n",
    "\n",
    "Key features:\n",
    "- Accepts both real and generated signals as input.\n",
    "- Uses convolutional layers for feature extraction.\n",
    "- Includes Virtual Batch Normalization for better training stability.\n",
    "- Outputs a probability score indicating whether the input is real or fake.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # D gets a noisy signal and clear signal as input [B x 2 x 16384]\n",
    "        negative_slope = 0.03\n",
    "        self.conv1 = nn.Conv1d(in_channels=2, out_channels=32, kernel_size=31, stride=2, padding=15)  # [B x 32 x 8192]\n",
    "        self.vbn1 = VirtualBatchNorm1d(32)\n",
    "        self.lrelu1 = nn.LeakyReLU(negative_slope)\n",
    "        self.conv2 = nn.Conv1d(32, 64, 31, 2, 15)  # [B x 64 x 4096]\n",
    "        self.vbn2 = VirtualBatchNorm1d(64)\n",
    "        self.lrelu2 = nn.LeakyReLU(negative_slope)\n",
    "        self.conv3 = nn.Conv1d(64, 64, 31, 2, 15)  # [B x 64 x 2048]\n",
    "        self.dropout1 = nn.Dropout()\n",
    "        self.vbn3 = VirtualBatchNorm1d(64)\n",
    "        self.lrelu3 = nn.LeakyReLU(negative_slope)\n",
    "        self.conv4 = nn.Conv1d(64, 128, 31, 2, 15)  # [B x 128 x 1024]\n",
    "        self.vbn4 = VirtualBatchNorm1d(128)\n",
    "        self.lrelu4 = nn.LeakyReLU(negative_slope)\n",
    "        self.conv5 = nn.Conv1d(128, 128, 31, 2, 15)  # [B x 128 x 512]\n",
    "        self.vbn5 = VirtualBatchNorm1d(128)\n",
    "        self.lrelu5 = nn.LeakyReLU(negative_slope)\n",
    "        self.conv6 = nn.Conv1d(128, 256, 31, 2, 15)  # [B x 256 x 256]\n",
    "        self.dropout2 = nn.Dropout()\n",
    "        self.vbn6 = VirtualBatchNorm1d(256)\n",
    "        self.lrelu6 = nn.LeakyReLU(negative_slope)\n",
    "        self.conv7 = nn.Conv1d(256, 256, 31, 2, 15)  # [B x 256 x 128]\n",
    "        self.vbn7 = VirtualBatchNorm1d(256)\n",
    "        self.lrelu7 = nn.LeakyReLU(negative_slope)\n",
    "        self.conv8 = nn.Conv1d(256, 512, 31, 2, 15)  # [B x 512 x 64]\n",
    "        self.vbn8 = VirtualBatchNorm1d(512)\n",
    "        self.lrelu8 = nn.LeakyReLU(negative_slope)\n",
    "        self.conv9 = nn.Conv1d(512, 512, 31, 2, 15)  # [B x 512 x 32]\n",
    "        self.dropout3 = nn.Dropout()\n",
    "        self.vbn9 = VirtualBatchNorm1d(512)\n",
    "        self.lrelu9 = nn.LeakyReLU(negative_slope)\n",
    "        self.conv10 = nn.Conv1d(512, 1024, 31, 2, 15)  # [B x 1024 x 16]\n",
    "        self.vbn10 = VirtualBatchNorm1d(1024)\n",
    "        self.lrelu10 = nn.LeakyReLU(negative_slope)\n",
    "        self.conv11 = nn.Conv1d(1024, 2048, 31, 2, 15)  # [B x 2048 x 8]\n",
    "        self.vbn11 = VirtualBatchNorm1d(2048)\n",
    "        self.lrelu11 = nn.LeakyReLU(negative_slope)\n",
    "        # 1x1 size kernel for dimension and parameter reduction\n",
    "        self.conv_final = nn.Conv1d(2048, 1, kernel_size=1, stride=1)  # [B x 1 x 8]\n",
    "        self.lrelu_final = nn.LeakyReLU(negative_slope)\n",
    "        self.fully_connected = nn.Linear(in_features=8, out_features=1)  # [B x 1]\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "        # initialize weights\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        \"\"\"\n",
    "        Initialize weights for convolution layers using Xavier initialization.\n",
    "        \"\"\"\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv1d):\n",
    "                nn.init.xavier_normal(m.weight.data)\n",
    "\n",
    "    def forward(self, x, ref_x):\n",
    "        \"\"\"\n",
    "        Forward pass of discriminator.\n",
    "\n",
    "        Args:\n",
    "            x: input batch (signal)\n",
    "            ref_x: reference input batch for virtual batch norm\n",
    "        \"\"\"\n",
    "        # reference pass\n",
    "        ref_x = self.conv1(ref_x)\n",
    "        ref_x, mean1, meansq1 = self.vbn1(ref_x, None, None)\n",
    "        ref_x = self.lrelu1(ref_x)\n",
    "        ref_x = self.conv2(ref_x)\n",
    "        ref_x, mean2, meansq2 = self.vbn2(ref_x, None, None)\n",
    "        ref_x = self.lrelu2(ref_x)\n",
    "        ref_x = self.conv3(ref_x)\n",
    "        ref_x = self.dropout1(ref_x)\n",
    "        ref_x, mean3, meansq3 = self.vbn3(ref_x, None, None)\n",
    "        ref_x = self.lrelu3(ref_x)\n",
    "        ref_x = self.conv4(ref_x)\n",
    "        ref_x, mean4, meansq4 = self.vbn4(ref_x, None, None)\n",
    "        ref_x = self.lrelu4(ref_x)\n",
    "        ref_x = self.conv5(ref_x)\n",
    "        ref_x, mean5, meansq5 = self.vbn5(ref_x, None, None)\n",
    "        ref_x = self.lrelu5(ref_x)\n",
    "        ref_x = self.conv6(ref_x)\n",
    "        ref_x = self.dropout2(ref_x)\n",
    "        ref_x, mean6, meansq6 = self.vbn6(ref_x, None, None)\n",
    "        ref_x = self.lrelu6(ref_x)\n",
    "        ref_x = self.conv7(ref_x)\n",
    "        ref_x, mean7, meansq7 = self.vbn7(ref_x, None, None)\n",
    "        ref_x = self.lrelu7(ref_x)\n",
    "        ref_x = self.conv8(ref_x)\n",
    "        ref_x, mean8, meansq8 = self.vbn8(ref_x, None, None)\n",
    "        ref_x = self.lrelu8(ref_x)\n",
    "        ref_x = self.conv9(ref_x)\n",
    "        ref_x = self.dropout3(ref_x)\n",
    "        ref_x, mean9, meansq9 = self.vbn9(ref_x, None, None)\n",
    "        ref_x = self.lrelu9(ref_x)\n",
    "        ref_x = self.conv10(ref_x)\n",
    "        ref_x, mean10, meansq10 = self.vbn10(ref_x, None, None)\n",
    "        ref_x = self.lrelu10(ref_x)\n",
    "        ref_x = self.conv11(ref_x)\n",
    "        ref_x, mean11, meansq11 = self.vbn11(ref_x, None, None)\n",
    "        # train pass\n",
    "        x = self.conv1(x)\n",
    "        x, _, _ = self.vbn1(x, mean1, meansq1)\n",
    "        x = self.lrelu1(x)\n",
    "        x = self.conv2(x)\n",
    "        x, _, _ = self.vbn2(x, mean2, meansq2)\n",
    "        x = self.lrelu2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.dropout1(x)\n",
    "        x, _, _ = self.vbn3(x, mean3, meansq3)\n",
    "        x = self.lrelu3(x)\n",
    "        x = self.conv4(x)\n",
    "        x, _, _ = self.vbn4(x, mean4, meansq4)\n",
    "        x = self.lrelu4(x)\n",
    "        x = self.conv5(x)\n",
    "        x, _, _ = self.vbn5(x, mean5, meansq5)\n",
    "        x = self.lrelu5(x)\n",
    "        x = self.conv6(x)\n",
    "        x = self.dropout2(x)\n",
    "        x, _, _ = self.vbn6(x, mean6, meansq6)\n",
    "        x = self.lrelu6(x)\n",
    "        x = self.conv7(x)\n",
    "        x, _, _ = self.vbn7(x, mean7, meansq7)\n",
    "        x = self.lrelu7(x)\n",
    "        x = self.conv8(x)\n",
    "        x, _, _ = self.vbn8(x, mean8, meansq8)\n",
    "        x = self.lrelu8(x)\n",
    "        x = self.conv9(x)\n",
    "        x = self.dropout3(x)\n",
    "        x, _, _ = self.vbn9(x, mean9, meansq9)\n",
    "        x = self.lrelu9(x)\n",
    "        x = self.conv10(x)\n",
    "        x, _, _ = self.vbn10(x, mean10, meansq10)\n",
    "        x = self.lrelu10(x)\n",
    "        x = self.conv11(x)\n",
    "        x, _, _ = self.vbn11(x, mean11, meansq11)\n",
    "        x = self.lrelu11(x)\n",
    "        x = self.conv_final(x)\n",
    "        x = self.lrelu_final(x)\n",
    "        # reduce down to a scalar value\n",
    "        x = torch.squeeze(x)\n",
    "        x = self.fully_connected(x)\n",
    "        return self.sigmoid(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Model Initialization and Training Loop\n",
    "\n",
    "the models (Discriminator and Generator) are initialized and moved to the GPU if available. The training loop begins, where both the generator and discriminator are alternately trained.\n",
    "\n",
    "- Initializes the discriminator and generator models.\n",
    "- Moves the models to the GPU if available.\n",
    "- Sets up optimizers for both the generator and discriminator using RMSprop.\n",
    "- Iterates over the specified number of epochs.\n",
    "- Within each epoch, it:\n",
    "  - Trains the discriminator to distinguish between clean and noisy audio.\n",
    "  - Trains the generator to produce clean audio from noisy input, based on feedback from the discriminator.\n",
    "  - Calculates loss for both the discriminator and generator, and updates their parameters accordingly.\n",
    "  - Displays training progress using `tqdm`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Arguments (adjustable through command line or notebook)\n",
    "BATCH_SIZE = 50  # Adjust as needed\n",
    "NUM_EPOCHS = 7  # Adjust as needed\n",
    "\n",
    "# Load data\n",
    "print('Loading data...')\n",
    "train_dataset = AudioDataset(data_type='train')\n",
    "test_dataset = AudioDataset(data_type='test')\n",
    "train_data_loader = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
    "test_data_loader = DataLoader(dataset=test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
    "\n",
    "# Generate reference batch\n",
    "ref_batch = train_dataset.reference_batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize models\n",
    "discriminator = Discriminator()\n",
    "generator = Generator()\n",
    "\n",
    "# Move models to GPU if available\n",
    "if torch.cuda.is_available():\n",
    "    discriminator.cuda()\n",
    "    generator.cuda()\n",
    "    ref_batch = ref_batch.cuda()\n",
    "\n",
    "ref_batch = Variable(ref_batch)\n",
    "\n",
    "print(\"# Generator parameters:\", sum(param.numel() for param in generator.parameters()))\n",
    "print(\"# Discriminator parameters:\", sum(param.numel() for param in discriminator.parameters()))\n",
    "\n",
    "# Optimizers\n",
    "g_optimizer = optim.RMSprop(generator.parameters(), lr=0.0001)\n",
    "d_optimizer = optim.RMSprop(discriminator.parameters(), lr=0.0001)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    train_bar = tqdm(train_data_loader)\n",
    "    for train_batch, train_clean, train_noisy in train_bar:\n",
    "        # Latent vector - normal distribution\n",
    "        z = nn.init.normal(torch.Tensor(train_batch.size(0), 1024, 8))\n",
    "        if torch.cuda.is_available():\n",
    "            train_batch, train_clean, train_noisy = train_batch.cuda(), train_clean.cuda(), train_noisy.cuda()\n",
    "            z = z.cuda()\n",
    "        train_batch, train_clean, train_noisy = Variable(train_batch), Variable(train_clean), Variable(train_noisy)\n",
    "        z = Variable(z)\n",
    "\n",
    "        # Train Discriminator to recognize clean audio as real\n",
    "        discriminator.zero_grad()\n",
    "        outputs = discriminator(train_batch, ref_batch)\n",
    "        clean_loss = torch.mean((outputs - 1.0) ** 2)  # L2 loss - we want them all to be 1\n",
    "        clean_loss.backward()\n",
    "\n",
    "        # Train Discriminator to recognize generated audio as noisy\n",
    "        generated_outputs = generator(train_noisy, z)\n",
    "        outputs = discriminator(torch.cat((generated_outputs, train_noisy), dim=1), ref_batch)\n",
    "        noisy_loss = torch.mean(outputs ** 2)  # L2 loss - we want them all to be 0\n",
    "        noisy_loss.backward()\n",
    "\n",
    "        # Update Discriminator\n",
    "        d_optimizer.step()\n",
    "\n",
    "        # Train Generator so that Discriminator recognizes generated audio as real\n",
    "        generator.zero_grad()\n",
    "        generated_outputs = generator(train_noisy, z)\n",
    "        gen_noise_pair = torch.cat((generated_outputs, train_noisy), dim=1)\n",
    "        outputs = discriminator(gen_noise_pair, ref_batch)\n",
    "\n",
    "        g_loss_ = 0.5 * torch.mean((outputs - 1.0) ** 2)\n",
    "        # L1 loss between generated output and clean sample\n",
    "        l1_dist = torch.abs(torch.add(generated_outputs, torch.neg(train_clean)))\n",
    "        g_cond_loss = 100 * torch.mean(l1_dist)  # Conditional loss\n",
    "        g_loss = g_loss_ + g_cond_loss\n",
    "\n",
    "        # Backprop + Optimize\n",
    "        g_loss.backward()\n",
    "        g_optimizer.step()\n",
    "\n",
    "        # Print training status\n",
    "        train_bar.set_description(\n",
    "            'Epoch {}: d_clean_loss {:.4f}, d_noisy_loss {:.4f}, g_loss {:.4f}, g_conditional_loss {:.4f}'\n",
    "            .format(epoch + 1, clean_loss.data[0], noisy_loss.data[0], g_loss.data[0], g_cond_loss.data[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Model Testing and Saving\n",
    "\n",
    "After training the model, this cell tests the generator by producing audio from noisy inputs in the test set. It saves the generated audio samples and the model parameters (generator and discriminator) after each epoch.\n",
    "\n",
    "- For each test batch, it generates enhanced audio using the trained generator.\n",
    "- Applies the `emphasis` function to enhance the audio further.\n",
    "- Saves the generated audio samples to the `results` directory.\n",
    "- Saves the generator and discriminator models after each epoch to the `epochs` directory.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Test model\n",
    "sample_rate = 16000\n",
    "test_bar = tqdm(test_data_loader, desc='Test model and save generated audios')\n",
    "for test_file_names, test_noisy in test_bar:\n",
    "    z = nn.init.normal(torch.Tensor(test_noisy.size(0), 1024, 8))\n",
    "    if torch.cuda.is_available():\n",
    "        test_noisy, z = test_noisy.cuda(), z.cuda()\n",
    "    test_noisy, z = Variable(test_noisy), Variable(z)\n",
    "    fake_speech = generator(test_noisy, z).data.cpu().numpy()  # Convert to numpy array\n",
    "    fake_speech = emphasis(fake_speech, emph_coeff=0.95, pre=False)\n",
    "\n",
    "    # Save generated audio samples\n",
    "    for idx in range(fake_speech.shape[0]):\n",
    "        generated_sample = fake_speech[idx]\n",
    "        file_name = os.path.join('results', '{}_e{}.wav'.format(test_file_names[idx].replace('.npy', ''), epoch + 1))\n",
    "        wavfile.write(file_name, sample_rate, generated_sample.T)\n",
    "\n",
    "# Save model parameters for each epoch\n",
    "g_path = os.path.join('epochs', 'generator-{}.pkl'.format(epoch + 1))\n",
    "d_path = os.path.join('epochs', 'discriminator-{}.pkl'.format(epoch + 1))\n",
    "torch.save(generator.state_dict(), g_path)\n",
    "torch.save(discriminator.state_dict(), d_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Audio Enhancement Using Pre-trained Generator Model \n",
    "\n",
    "This code enhances a noisy audio file by applying a pre-trained generator model. The process involves several steps outlined below:\n",
    "\n",
    "1. **Argument Parsing**:\n",
    "   - The script uses `argparse` to accept two command-line arguments: \n",
    "     - `file_name`: The path to the noisy audio file that needs enhancement.\n",
    "     - `epoch_name`: The name of the generator model checkpoint to be used for enhancement.\n",
    "\n",
    "2. **Loading the Pre-trained Model**:\n",
    "   - The `Generator` model is instantiated and loaded with the weights of the specified epoch.\n",
    "   - If a GPU is available, the model is moved to the GPU for efficient computation.\n",
    "\n",
    "3. **Audio Preprocessing**:\n",
    "   - The noisy audio file is sliced into smaller segments using the `segment_audio_file` function. These smaller segments are easier to process individually by the model.\n",
    "\n",
    "4. **Audio Enhancement**:\n",
    "   - Each slice of the noisy audio is passed through the generator model for enhancement.\n",
    "   - A noise vector `z` is generated and used as input alongside the noisy audio slice.\n",
    "   - The `emphasis` function is applied both before and after enhancement to improve the audio quality.\n",
    "\n",
    "5. **Saving the Enhanced Audio**:\n",
    "   - After processing all slices, the enhanced audio is recombined into a continuous waveform.\n",
    "   - The enhanced audio is saved as a `.wav` file with a filename that includes the prefix `enhanced_` followed by the original filename.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define file path and epoch name for the generator\n",
    "FILE_NAME = \"/home/antec/Desktop/Najjar/SEGAN-master/data/noisy_wav/p232_002.wav\"  # Replace with your actual file path\n",
    "EPOCH_NAME = \"EPOCH_NAME = /home/antec/Desktop/Najjar/SEGAN-master/checkpoints/checkpoint_epoch7_20250109_180511.pt\"\n",
    "\n",
    "# Load the pre-trained generator model\n",
    "generator = Generator()\n",
    "generator.load_state_dict(torch.load(f'epochs/{EPOCH_NAME}', map_location='cpu'))\n",
    "if torch.cuda.is_available():\n",
    "    generator.cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slice the noisy audio file into smaller segments\n",
    "WINDOW_SIZE = 2 ** 14  # Approx. 1 second\n",
    "SAMPLE_RATE = 16000\n",
    "STRIDE = 0.5  \n",
    "noisy_slices = segment_audio_file(FILE_NAME, WINDOW_SIZE, 1, sample_rate)\n",
    "enhanced_speech = []\n",
    "\n",
    "# Process each slice through the generator model\n",
    "for noisy_slice in tqdm(noisy_slices, desc='Generate enhanced audio'):\n",
    "    z = nn.init.normal(torch.Tensor(1, 1024, 8))  # Generate random noise vector\n",
    "    noisy_slice = torch.from_numpy(emphasis(noisy_slice[np.newaxis, np.newaxis, :])).type(torch.FloatTensor)\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        noisy_slice, z = noisy_slice.cuda(), z.cuda()\n",
    "    \n",
    "    noisy_slice, z = Variable(noisy_slice), Variable(z)\n",
    "    generated_speech = generator(noisy_slice, z).data.cpu().numpy()\n",
    "    generated_speech = emphasis(generated_speech, emph_coeff=0.95, pre=False)\n",
    "    generated_speech = generated_speech.reshape(-1)\n",
    "    enhanced_speech.append(generated_speech)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the enhanced audio as a .wav file\n",
    "enhanced_speech = np.array(enhanced_speech).reshape(1, -1)\n",
    "file_name = os.path.join(os.path.dirname(FILE_NAME),\n",
    "                         f'enhanced_{os.path.basename(FILE_NAME).split(\".\")[0]}.wav')\n",
    "wavfile.write(file_name, sample_rate, enhanced_speech.T)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
